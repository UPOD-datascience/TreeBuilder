{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.abspath('.'),'..', 'src'))\n",
    "import tree_utils, ctree\n",
    "import pickle\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Lasso, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e736ae5700a4dfc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:36:39.940453Z",
     "start_time": "2024-09-02T15:36:39.774448Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../artifacts/data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87dc36addd60da",
   "metadata": {},
   "source": [
    "```\n",
    "We’d like to show in the tree-visualization the names for the outcomes as follows: \n",
    "-\t[X] In the conduction/muscle trees: SR should be ‘control’\n",
    "-\t[X] In the axis tree, ‘ normal axis’ should not be ‘ control’ , but just ‘ normal axis’ \n",
    "-\t[X] In the conduction tree, ‘BF’ should be ‘ BfB’ \n",
    "\n",
    "The trees that we will show and are thus the most important:\n",
    "1.\tConduction: Customized tree (no missing indicator features, with the morphology maps as we defined the 4 categories, with the customization of the QRS duration (of 110 and 120 ms as done before) \n",
    "2.\tAxis/muscle: semi-customized (only use the  features of the selection that we provided, no missing indicator features, with the morphology maps in 4 categories) \n",
    "\n",
    "We’ll compare in the ROC-curves, and the net benefit curves 3 models, so it would be great if we could have the net benefit curves with the following combinations of models (the ROC-curve figures, we can make ourselves once we have the results of the new trees).  \n",
    "-\tConduction: \n",
    "o\t1. xgb 2. lr 3. dt 4. Customized dt (the decision tree being the customized one as described in point 1 before)\n",
    "-\tAxis/muscle: \n",
    "o\t1.xgb 2. lr 3. Semi-customized dt (the decision tree being the semi-customized one as described in point 2 before)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba2273109837cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphology_categories = {\n",
    "    'only positive, no notch/acc': ['R'], \n",
    "    'only negative, no notch/acc': ['S'],\n",
    "    'both positive and negative, no notch/acc': ['Q.R', 'Q.R.S', 'R.S'],\n",
    "    'only positive with notch/accent': [\n",
    "        'R.R_acc', 'R.Rn', 'R.Rn.R_acc', 'Rn.R', 'Rn.R.R_acc', 'Rn.R.Rn'\n",
    "    ],\n",
    "    'only negative with notch/accent': [\n",
    "        'S.Sn', 'Sn.S', 'Sn.S.Sn'\n",
    "    ],\n",
    "    'both positive and negative with notch/accent': [\n",
    "        'Q.R.R_acc', 'Q.R.R_acc.S', 'Q.R.Rn', 'Q.R.Rn.S', 'Q.R.S.R_acc',\n",
    "        'Q.R.S.R_acc.S_acc', 'Q.R.S.Sn', 'Q.Rn.R', 'Q.Rn.R.S', 'R.R_acc.S',\n",
    "        'R.R_acc.S.S_acc', 'R.Rn.S', 'R.S.R_acc', 'R.S.R_acc.S_acc', 'R.S.Rn',\n",
    "        'R.S.Rn.Sn', 'R.S.S_acc', 'R.S.Sn', 'R.Sn.S', 'R.Sn.S.R_acc',\n",
    "        'R.Sn.S.Sn', 'Rn.R.R_acc.S', 'Rn.R.Rn.S', 'Rn.R.S', 'Rn.R.S.R_acc',\n",
    "        'Rn.R.S.R_acc.S_acc', 'Rn.R.S.Rn'\n",
    "    ],\n",
    "    'none': ['none']\n",
    "}\n",
    "inv_morpho_map = {_v:k  for k,v in morphology_categories.items() for _v in v}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fd92d0c0f1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'J:\\Onderzoek\\21-763_rvanes_MiniECG-2-Data\\E_ResearchData\\2_ResearchData\\Parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553efb423d7da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "NameMap = pd.read_parquet(os.path.join(data_dir, '..', 'Name_toSimpleName.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dcb45d9b1223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NameMapDict = {k:v for k,v in zip(NameMap['Old_Name'].values, NameMap['New_Name'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6219ba685f490c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MORPHO_PRESENCE = 0.15 # %\n",
    "MULTI_CLASS = False\n",
    "num_splits = 10\n",
    "num_repeats = 10\n",
    "MISSINGNESS_INDICATOR = False\n",
    "MORPHO_MAP = True\n",
    "\n",
    "MULTI_CLASS_STRING = \"_MultiClass\" if MULTI_CLASS else \"_BinaryClass\"\n",
    "MISSINGNESS_INDICATOR_STRING = \"_wMissing\" if MISSINGNESS_INDICATOR else \"\"\n",
    "MORPHO_MAP_STRING = \"_wMorphoMap\" if MORPHO_MAP else \"\"\n",
    "\n",
    "TARGET = \"muscle\" # axis, muscle, conduction\n",
    "rules_path = f'T://laupodteam/AIOS/Bram/notebooks/code_dev/miniECG_interpretation/TreeBuilder/assets/{TARGET}_tree.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa44efa6aff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(r'J:\\Onderzoek\\21-763_rvanes_MiniECG-2-Data\\G_Output\\2_Data\\CustomTree', f'{TARGET}{MULTI_CLASS_STRING}')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec277f5dd147445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET=='conduction':\n",
    "    rules_loader = ctree.LoadRules(rules_path, name_map=NameMapDict)\n",
    "    processed_rules = rules_loader.get_processed_rules()\n",
    "    \n",
    "    SplitColumn = rules_loader.fold_split_col\n",
    "    TargetCol = rules_loader.target_col\n",
    "    IgnoreCols = rules_loader.ignore_cols + [SplitColumn]\n",
    "    FeaturesToUse = rules_loader.features_to_use\n",
    "else:\n",
    "    rules_loader = None\n",
    "    processed_rules = None\n",
    "    rules_loader_dict = json.load(open(rules_path, 'r'))\n",
    "    \n",
    "    SplitColumn = rules_loader_dict['fold_split_col']\n",
    "    TargetCol = rules_loader_dict['target_col']\n",
    "    IgnoreCols = rules_loader_dict['ignore_cols'] + [SplitColumn]\n",
    "    FeaturesToUse = rules_loader_dict['features_to_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a537c95bdb9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_parquet(os.path.join(data_dir, f'DATA.parquet'))\n",
    "\n",
    "if len(FeaturesToUse)>0:\n",
    "    keep_columns = list(set(FeaturesToUse).difference(set(IgnoreCols)))\n",
    "else:\n",
    "    keep_columns = [c for c in DATA.columns if c not in IgnoreCols]\n",
    "    \n",
    "keep_columns = list(set(keep_columns+[TargetCol]))\n",
    "    \n",
    "DATA = DATA.loc[:, keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462f88b69787038",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns = [NameMapDict[c] for c in DATA.columns]\n",
    "morphology_columns = [c for c in DATA.columns if 'morphology' in c.lower()]\n",
    "lead_columns = [c for c in DATA.columns if ('lead' in c.lower()) & ('morphology' not in c.lower())]\n",
    "for c in morphology_columns:\n",
    "    DATA.loc[:, c] = DATA[c].apply(lambda x: x[0].strip(\",\").strip(\" \"))\n",
    "    DATA.loc[:, c] = DATA[c].apply(lambda x: x if x.strip()!=\"\" else \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4908b9039d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MORPHO_MAP:\n",
    "    for c in morphology_columns:\n",
    "        DATA.loc[:, c] = DATA[c].map(inv_morpho_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ee896ce7264c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for lOl in [DATA[c].str.split(\".\").values for c in morphology_columns]:\n",
    "    for l in lOl:\n",
    "        for _s in l:\n",
    "            vocab.add(_s)\n",
    "Vocab = {k:v for k,v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3cc9646303824",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHot = OneHotEncoder(drop=None, \n",
    "                       sparse_output=False, \n",
    "                       min_frequency=MIN_MORPHO_PRESENCE,\n",
    "                       handle_unknown='infrequent_if_exist')\n",
    "\n",
    "MorphologyOneHot = pd.DataFrame(data=OneHot.fit_transform(DATA[morphology_columns]), \n",
    "                            columns=OneHot.get_feature_names_out(morphology_columns),\n",
    "                            index=DATA.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88851f9624ac3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA.drop(morphology_columns, axis=1)\n",
    "DATA = pd.concat([DATA, MorphologyOneHot], axis=1)\n",
    "keep_columns = DATA.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289fc3327ed1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Make multiple DATA, X,Y for: AXIS, MUSCLE and CONDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c714dfc36621795",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET == 'conduction':\n",
    "    DATA = DATA.assign(Diagnosis=DATA.Diagnosis.map({\n",
    "                                                                    'SR': 'Control',\n",
    "                                                                    'BF': 'BfB',\n",
    "                                                                    'RBBB': 'RBBB',\n",
    "                                                                    'LBBB': 'LBBB',\n",
    "                                                                    'LAFB': 'LAFB',\n",
    "                                                                    'LAFB , LVH': 'LAFB',\n",
    "                                                                    'Microvoltages , BF': 'BfB',\n",
    "                                                                    'Microvoltages , RBBB': 'RBBB',\n",
    "                                                                    'Microvoltages , LAFB': 'LAFB', \n",
    "                                                                    'LVH , BF': 'BfB',\n",
    "                                                                    'LVH , RBBB': 'RBBB',\n",
    "                                                                    'LVH , LBBB': 'LBBB'\n",
    "                                                                }))\n",
    "    Reduction_map = {'BfB': 'Abnormal', \n",
    "                     'LBBB': 'Abnormal', \n",
    "                     'RBBB': 'Abnormal',\n",
    "                     'LAFB': 'Abnormal',\n",
    "                     'Control': 'Control'}\n",
    "elif TARGET == 'axis':\n",
    "    if MULTI_CLASS:\n",
    "        target_inclusion = ['Left', 'Normal', 'Right']\n",
    "    else:\n",
    "        target_inclusion = ['Left', 'Normal', 'Right', 'Extreme']\n",
    "    DATA = DATA.loc[DATA['Heart Axis Diagnosis'].isin(target_inclusion)]    \n",
    "    Reduction_map = {'Left': 'Abnormal', \n",
    "                     'Right': 'Abnormal',\n",
    "                     'Extreme': 'Abnormal',\n",
    "                     'Normal': 'Normal'}   \n",
    "elif TARGET == 'muscle':\n",
    "    DATA = DATA.assign(Diagnosis=DATA.Diagnosis.map({\n",
    "                                                            'SR': 'Control',\n",
    "                                                            'Microvoltages': 'Microvoltages',\n",
    "                                                            'LVH': 'LVH',\n",
    "                                                            'LAFB , LVH': 'LVH',\n",
    "                                                            'Microvoltages , BF': 'Microvoltages',\n",
    "                                                            'Microvoltages , RBBB': 'Microvoltages',\n",
    "                                                            'Microvoltages , LAFB': 'Microvoltages',\n",
    "                                                            'LVH , BF': 'LVH',\n",
    "                                                            'LVH , RBBB': 'LVH',\n",
    "                                                            'LVH , LBBB': 'LVH'\n",
    "                                                        }))\n",
    "    target_inclusion = ['Control','LVH','Microvoltages']\n",
    "    DATA = DATA.loc[DATA['Diagnosis'].isin(target_inclusion)]    \n",
    "\n",
    "    Reduction_map = {'Microvoltages': 'Abnormal', \n",
    "                     'LVH': 'Abnormal',\n",
    "                     'Control': 'Control'}\n",
    "else:\n",
    "    raise ValueError(f'Unknown target {TARGET}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88667be8a69e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MULTI_CLASS==False:\n",
    "    DATA.loc[:, TargetCol] = DATA[TargetCol].map(Reduction_map)\n",
    "DATA = DATA.dropna(subset=[TargetCol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc59f592300e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Infreq_cat_dict = {morphology_columns[k]:list(inf_cats) for k, inf_cats in enumerate(OneHot.infrequent_categories_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647cddbf1921ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(Infreq_cat_dict, open(os.path.join(output_dir, 'infrequent_categories_map.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e34320264c5225",
   "metadata": {},
   "source": [
    "# Make tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705c65e4096a8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TreeKwargs = {\n",
    "    'criterion':'gini', \n",
    "    'splitter':'best', \n",
    "    'max_depth':5, \n",
    "    'min_samples_split':10, \n",
    "    'min_samples_leaf': 5, \n",
    "    'min_weight_fraction_leaf':0.05, \n",
    "    'max_features':None, \n",
    "    'random_state':7, \n",
    "    'max_leaf_nodes':50,\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "xgboost_kwargs = {\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': 6,\n",
    "    'max_leaves': 50,\n",
    "    'learning_rate': 2e-3,\n",
    "    'gamma': 0.4,\n",
    "    'subsample': 0.55,\n",
    "    'colsample_bytree':0.85,\n",
    "    'reg_alpha': 0.005,\n",
    "}\n",
    "logistic_kwargs = {\n",
    "    'penalty': 'elasticnet', \n",
    "    'solver': 'saga', \n",
    "    'dual': False, \n",
    "    'tol': 0.0001, \n",
    "    'C':1.0, \n",
    "    'fit_intercept': True, \n",
    "    'intercept_scaling':1, \n",
    "    'class_weight':None, \n",
    "    'random_state':7,     \n",
    "    'max_iter':3000, \n",
    "    'verbose': 0, \n",
    "    'warm_start': False, \n",
    "    'n_jobs':-1, \n",
    "    'l1_ratio':0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a47f3113df9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "Splitter = RepeatedStratifiedKFold(n_splits=num_splits, \n",
    "                                   n_repeats=num_repeats, \n",
    "                                   random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d125a4d2c9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DATA.loc[:,[c for c in keep_columns if c!=TargetCol]]\n",
    "Y = DATA[TargetCol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fe0a87ee2e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(Y)\n",
    "TargetMap = {k:v for k,v in enumerate(lbe.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944ee82a01ff1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetMap.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74272d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50139353f8bb4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_parquet(os.path.join(output_dir, f'data{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e244d313d5c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for i, (train_index, test_index) in tqdm.tqdm(enumerate(Splitter.split(X, Y)),\n",
    "                                              total=num_splits * num_repeats):\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    y_train_encoded = lbe.transform(Y_train)\n",
    "    y_test_encoded = lbe.transform(Y_test)\n",
    "        \n",
    "    #Imputer = IterativeImputer(Lasso(), \n",
    "    #                           add_indicator=MISSINGNESS_INDICATOR, \n",
    "    #                           max_iter=5_000, verbose=0)\n",
    "    Imputer = KNNImputer(add_indicator=MISSINGNESS_INDICATOR, n_neighbors=10, weights='distance')\n",
    "    \n",
    "    Imputer.fit(X_train)\n",
    "    \n",
    "    X_train_imputed = Imputer.transform(X_train)\n",
    "    X_test_imputed = Imputer.transform(X_test)\n",
    "    \n",
    "    X_train_imputed = pd.DataFrame(data=X_train_imputed,\n",
    "                                   columns=Imputer.get_feature_names_out())\n",
    "    \n",
    "    X_test_imputed = pd.DataFrame(data=X_test_imputed,\n",
    "                                   columns=Imputer.get_feature_names_out())\n",
    "    \n",
    "    clf = ctree.CustomDecisionTree(custom_rules=processed_rules,\n",
    "                             prune_threshold=None,\n",
    "                             Tree_kwargs=TreeKwargs,\n",
    "                             TargetMap = TargetMap, \n",
    "                             tot_max_depth=5)\n",
    "    if processed_rules is not None:\n",
    "        #print(\"Training custom tree...\")\n",
    "        clf.fit(X_train_imputed, y_train_encoded)\n",
    "        enriched_rules = clf.get_enriched_rules()\n",
    "        final_tree = clf.get_custom_rules_model()\n",
    "        \n",
    "        #############################\n",
    "        ## Writing out the tree #####\n",
    "        #############################\n",
    "        \n",
    "        if processed_rules is not None:\n",
    "            json.dump(final_tree, \n",
    "                      open(os.path.join(output_dir, f\"tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.json\"), mode='w'))\n",
    "            ctree.update_html(tree=final_tree, \n",
    "                              html_path=\"../src/treeTemplate.html\", \n",
    "                              output_path=os.path.join(output_dir, f\"tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.html\"))\n",
    "        cust_probas_train = clf.predict_proba(X_train_imputed)\n",
    "        cust_probas_test = clf.predict_proba(X_test_imputed)\n",
    "\n",
    "\n",
    "    clf_base = DecisionTreeClassifier(**TreeKwargs)\n",
    "    clf_xgb = XGBClassifier(**xgboost_kwargs)\n",
    "    clf_logistic = LogisticRegression(**logistic_kwargs)\n",
    "    \n",
    "    #print(\"Training classifiers...\")\n",
    "    #print(\"Training standard decision tree...\")\n",
    "    clf_base.fit(X_train_imputed, y_train_encoded)\n",
    "    #print(\"Training xgboost...\")\n",
    "    clf_xgb.fit(X_train_imputed, y_train_encoded)\n",
    "    #print(\"Training logistic regression...\")\n",
    "    clf_logistic.fit(X_train_imputed, y_train_encoded)\n",
    "    \n",
    "    Fold = i % num_splits\n",
    "    Repeat = i // num_splits\n",
    "    \n",
    "    #############################\n",
    "    ## Writing out the tree #####\n",
    "    #############################\n",
    "    \n",
    "    sklearn_tree = clf.load_from_sklearn_tree(clf_base, X_train_imputed, y_train_encoded)\n",
    "    final_tree_sklearn = sklearn_tree.get_custom_rules_model()\n",
    "    json.dump(final_tree_sklearn, \n",
    "              open(os.path.join(output_dir, f\"sklearn_tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.json\"), mode='w'))\n",
    "    ctree.update_html(tree=final_tree_sklearn, \n",
    "                     html_path=\"../src/treeTemplate.html\", \n",
    "                     output_path=os.path.join(output_dir, f\"sklearn_tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.html\"))\n",
    "    #############################\n",
    "    #############################\n",
    "    #############################    \n",
    "\n",
    "    \n",
    "    base_probas_train = clf_base.predict_proba(X_train_imputed)\n",
    "    base_probas_test = clf_base.predict_proba(X_test_imputed)\n",
    "    \n",
    "    xgb_probas_train = clf_xgb.predict_proba(X_train_imputed)\n",
    "    xgb_probas_test = clf_xgb.predict_proba(X_test_imputed)\n",
    "    \n",
    "    logistic_probas_train = clf_logistic.predict_proba(X_train_imputed)\n",
    "    logistic_probas_test = clf_logistic.predict_proba(X_test_imputed)\n",
    "    \n",
    "    result_df['indices'] = np.hstack([train_index, test_index])\n",
    "    result_df['Fold'] = Fold\n",
    "    result_df['Repeat'] = Repeat\n",
    "    result_df['Y_true'] = np.hstack([Y_train.values, Y_test.values])\n",
    "    result_df[[f'Y_pred_normalDT_{cname}' for cname in TargetMap.values()]] = np.vstack([base_probas_train, base_probas_test])\n",
    "    if processed_rules is not None:\n",
    "        result_df[[f'Y_pred_customDT_{cname}' for cname in TargetMap.values()]] = np.vstack([cust_probas_train, cust_probas_test])\n",
    "    result_df[[f'Y_pred_XGB_{cname}' for cname in TargetMap.values()]] = np.vstack([xgb_probas_train, xgb_probas_test])\n",
    "    result_df[[f'Y_pred_LR_{cname}' for cname in TargetMap.values()]] = np.vstack([logistic_probas_train, logistic_probas_test])    \n",
    "    result_df['Dataset'] = ['train' for _ in train_index]+['test' for _ in test_index]\n",
    "    \n",
    "    results_list.append(result_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb23977f46fe04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_results = pd.concat(results_list, axis=0, ignore_index=True)\n",
    "Fina_results = Final_results.reset_index(drop=True)\n",
    "\n",
    "classes = set([c.split(\"_\")[-1] for c in Final_results.columns if 'pred' in c])\n",
    "\n",
    "for _class in classes:\n",
    "    Final_results[f'Y_true_{_class}'] = (Final_results['Y_true'] == _class).astype(int)\n",
    "\n",
    "Final_results.to_csv(\n",
    "    os.path.join(output_dir, f\"results{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.csv\"),\n",
    "    index=False, sep=\";\")\n",
    "\n",
    "Final_results.to_parquet(\n",
    "    os.path.join(output_dir, f\"results{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4546e88efb16d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
