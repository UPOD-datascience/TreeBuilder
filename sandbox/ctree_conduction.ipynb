{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:53:09.516234Z",
     "start_time": "2024-10-31T13:53:09.379144Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.abspath('.'),'..', 'src'))\n",
    "import tree_utils, ctree\n",
    "import pickle\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Lasso, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.multiclass import OneVsRestClassifier as OvR\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e736ae5700a4dfc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:04.058136Z"
    }
   },
   "outputs": [],
   "source": [
    "#data = pd.read_parquet(r'J:\\Onderzoek\\21-763_rvanes_MiniECG-2-Data\\E_ResearchData\\2_ResearchData\\Parquet\\DATA.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87dc36addd60da",
   "metadata": {},
   "source": [
    "```\n",
    "We’d like to show in the tree-visualization the names for the outcomes as follows: \n",
    "-\t[X] In the conduction/muscle trees: SR should be ‘control’\n",
    "-\t[X] In the axis tree, ‘ normal axis’ should not be ‘ control’ , but just ‘ normal axis’ \n",
    "-\t[X] In the conduction tree, ‘BF’ should be ‘ BfB’ \n",
    "\n",
    "The trees that we will show and are thus the most important:\n",
    "1.\tConduction: Customized tree (no missing indicator features, with the morphology maps as we defined the 4 categories, with the customization of the QRS duration (of 110 and 120 ms as done before) \n",
    "2.\tAxis/muscle: semi-customized (only use the  features of the selection that we provided, no missing indicator features, with the morphology maps in 4 categories) \n",
    "\n",
    "We’ll compare in the ROC-curves, and the net benefit curves 3 models, so it would be great if we could have the net benefit curves with the following combinations of models (the ROC-curve figures, we can make ourselves once we have the results of the new trees).  \n",
    "-\tConduction: \n",
    "o\t1. xgb 2. lr 3. dt 4. Customized dt (the decision tree being the customized one as described in point 1 before)\n",
    "-\tAxis/muscle: \n",
    "o\t1.xgb 2. lr 3. Semi-customized dt (the decision tree being the semi-customized one as described in point 2 before)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ba2273109837cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:04.274650Z"
    }
   },
   "outputs": [],
   "source": [
    "morphology_categories = {\n",
    "    'only positive, no notch/acc': ['R'], \n",
    "    'only negative, no notch/acc': ['S'],\n",
    "    'both positive and negative, no notch/acc': ['Q.R', 'Q.R.S', 'R.S'],\n",
    "    'only positive with notch/accent': [\n",
    "        'R.R_acc', 'R.Rn', 'R.Rn.R_acc', 'Rn.R', 'Rn.R.R_acc', 'Rn.R.Rn'\n",
    "    ],\n",
    "    'only negative with notch/accent': [\n",
    "        'S.Sn', 'Sn.S', 'Sn.S.Sn'\n",
    "    ],\n",
    "    'both positive and negative with notch/accent': [\n",
    "        'Q.R.R_acc', 'Q.R.R_acc.S', 'Q.R.Rn', 'Q.R.Rn.S', 'Q.R.S.R_acc',\n",
    "        'Q.R.S.R_acc.S_acc', 'Q.R.S.Sn', 'Q.Rn.R', 'Q.Rn.R.S', 'R.R_acc.S',\n",
    "        'R.R_acc.S.S_acc', 'R.Rn.S', 'R.S.R_acc', 'R.S.R_acc.S_acc', 'R.S.Rn',\n",
    "        'R.S.Rn.Sn', 'R.S.S_acc', 'R.S.Sn', 'R.Sn.S', 'R.Sn.S.R_acc',\n",
    "        'R.Sn.S.Sn', 'Rn.R.R_acc.S', 'Rn.R.Rn.S', 'Rn.R.S', 'Rn.R.S.R_acc',\n",
    "        'Rn.R.S.R_acc.S_acc', 'Rn.R.S.Rn'\n",
    "    ],\n",
    "    'none': ['none']\n",
    "}\n",
    "inv_morpho_map = {_v:k  for k,v in morphology_categories.items() for _v in v}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328fd92d0c0f1da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:04.493135Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = r'J:\\Onderzoek\\21-763_rvanes_MiniECG-2-Data\\E_ResearchData\\2_ResearchData\\Parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553efb423d7da36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:04.693139Z"
    }
   },
   "outputs": [],
   "source": [
    "NameMap = pd.read_parquet(os.path.join(data_dir, '..', 'Name_toSimpleName.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59dcb45d9b1223c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:04.894140Z"
    }
   },
   "outputs": [],
   "source": [
    "NameMapDict = {k:v for k,v in zip(NameMap['Old_Name'].values, NameMap['New_Name'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6219ba685f490c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:05.111644Z"
    }
   },
   "outputs": [],
   "source": [
    "MIN_MORPHO_PRESENCE = 0.15 # %\n",
    "MULTI_CLASS = False\n",
    "num_splits = 2\n",
    "num_repeats = 1\n",
    "MISSINGNESS_INDICATOR = False\n",
    "MORPHO_MAP = True\n",
    "WRITE_OUT = True\n",
    "\n",
    "MULTI_CLASS_STRING = \"_MultiClass\" if MULTI_CLASS else \"_BinaryClass\"\n",
    "MISSINGNESS_INDICATOR_STRING = \"_wMissing\" if MISSINGNESS_INDICATOR else \"\"\n",
    "MORPHO_MAP_STRING = \"_wMorphoMap\" if MORPHO_MAP else \"\"\n",
    "\n",
    "VERSION = 'v.3'\n",
    "\n",
    "TARGET = \"muscle\" # axis, muscle, conduction\n",
    "rules_path = f'T://laupodteam/AIOS/Bram/notebooks/library_dev/TreeBuilder/assets/{TARGET}_tree.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fa44efa6aff753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:05.328648Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = os.path.join(r'J:\\Onderzoek\\21-763_rvanes_MiniECG-2-Data\\G_Output\\2_Data\\CustomTree', f'{TARGET}{MULTI_CLASS_STRING}{VERSION}')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec277f5dd147445a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:05.917166Z"
    }
   },
   "outputs": [],
   "source": [
    "if TARGET=='conduction':\n",
    "    rules_loader = ctree.LoadRules(rules_path, name_map=NameMapDict)\n",
    "    processed_rules = rules_loader.get_processed_rules()\n",
    "    \n",
    "    SplitColumn = rules_loader.fold_split_col\n",
    "    TargetCol = rules_loader.target_col\n",
    "    IgnoreCols = rules_loader.ignore_cols + [SplitColumn]\n",
    "    FeaturesToUse = rules_loader.features_to_use\n",
    "else:\n",
    "    rules_loader = None\n",
    "    processed_rules = None\n",
    "    rules_loader_dict = json.load(open(rules_path, 'r'))\n",
    "    \n",
    "    SplitColumn = rules_loader_dict['fold_split_col']\n",
    "    TargetCol = rules_loader_dict['target_col']\n",
    "    IgnoreCols = rules_loader_dict['ignore_cols'] + [SplitColumn]\n",
    "    FeaturesToUse = rules_loader_dict['features_to_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a537c95bdb9d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:06.122172Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = pd.read_parquet(os.path.join(data_dir, f'DATA.parquet'))\n",
    "\n",
    "if len(FeaturesToUse)>0:\n",
    "    keep_columns = list(set(FeaturesToUse).difference(set(IgnoreCols)))\n",
    "else:\n",
    "    keep_columns = [c for c in DATA.columns if c not in IgnoreCols]\n",
    "    \n",
    "keep_columns = list(set(keep_columns+[TargetCol]))\n",
    "    \n",
    "DATA = DATA.loc[:, keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8462f88b69787038",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:06.445164Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA.columns = [NameMapDict[c] for c in DATA.columns]\n",
    "morphology_columns = [c for c in DATA.columns if 'morphology' in c.lower()]\n",
    "lead_columns = [c for c in DATA.columns if ('lead' in c.lower()) & ('morphology' not in c.lower())]\n",
    "for c in morphology_columns:\n",
    "    DATA.loc[:, c] = DATA[c].apply(lambda x: x[0].strip(\",\").strip(\" \"))\n",
    "    DATA.loc[:, c] = DATA[c].apply(lambda x: x if x.strip()!=\"\" else \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc4908b9039d4ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:06.723668Z"
    }
   },
   "outputs": [],
   "source": [
    "if MORPHO_MAP:\n",
    "    for c in morphology_columns:\n",
    "        DATA.loc[:, c] = DATA[c].map(inv_morpho_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "713ee896ce7264c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:06.971673Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for lOl in [DATA[c].str.split(\".\").values for c in morphology_columns]:\n",
    "    for l in lOl:\n",
    "        for _s in l:\n",
    "            vocab.add(_s)\n",
    "Vocab = {k:v for k,v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe3cc9646303824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:07.204177Z"
    }
   },
   "outputs": [],
   "source": [
    "OneHot = OneHotEncoder(drop=None, \n",
    "                       sparse_output=False, \n",
    "                       min_frequency=MIN_MORPHO_PRESENCE,\n",
    "                       handle_unknown='infrequent_if_exist')\n",
    "\n",
    "MorphologyOneHot = pd.DataFrame(data=OneHot.fit_transform(DATA[morphology_columns]), \n",
    "                            columns=OneHot.get_feature_names_out(morphology_columns),\n",
    "                            index=DATA.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88851f9624ac3ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.352908500Z",
     "start_time": "2024-09-12T14:11:07.468678Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = DATA.drop(morphology_columns, axis=1)\n",
    "DATA = pd.concat([DATA, MorphologyOneHot], axis=1)\n",
    "keep_columns = DATA.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2289fc3327ed1cfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:07.684180Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Make multiple DATA, X,Y for: AXIS, MUSCLE and CONDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c714dfc36621795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:07.901685Z"
    }
   },
   "outputs": [],
   "source": [
    "if TARGET == 'conduction':\n",
    "    DATA = DATA.assign(Diagnosis=DATA.Diagnosis.map({\n",
    "                                                                    'SR': 'Control',\n",
    "                                                                    'BF': 'BfB',\n",
    "                                                                    'RBBB': 'RBBB',\n",
    "                                                                    'LBBB': 'LBBB',\n",
    "                                                                    'LAFB': 'LAFB',\n",
    "                                                                    'LAFB , LVH': 'LAFB',\n",
    "                                                                    'Microvoltages , BF': 'BfB',\n",
    "                                                                    'Microvoltages , RBBB': 'RBBB',\n",
    "                                                                    'Microvoltages , LAFB': 'LAFB', \n",
    "                                                                    'LVH , BF': 'BfB',\n",
    "                                                                    'LVH , RBBB': 'RBBB',\n",
    "                                                                    'LVH , LBBB': 'LBBB'\n",
    "                                                                }))\n",
    "    Reduction_map = {'BfB': 'Abnormal', \n",
    "                     'LBBB': 'Abnormal', \n",
    "                     'RBBB': 'Abnormal',\n",
    "                     'LAFB': 'Abnormal',\n",
    "                     'Control': 'Control'}\n",
    "elif TARGET == 'axis':\n",
    "    if MULTI_CLASS:\n",
    "        target_inclusion = ['Left', 'Normal', 'Right']\n",
    "    else:\n",
    "        target_inclusion = ['Left', 'Normal', 'Right', 'Extreme']\n",
    "    DATA = DATA.loc[DATA['Heart Axis Diagnosis'].isin(target_inclusion)]    \n",
    "    Reduction_map = {'Left': 'Abnormal', \n",
    "                     'Right': 'Abnormal',\n",
    "                     'Extreme': 'Abnormal',\n",
    "                     'Normal': 'Normal'}   \n",
    "elif TARGET == 'muscle':\n",
    "    DATA = DATA.assign(Diagnosis=DATA.Diagnosis.map({\n",
    "                                                            'SR': 'Control',\n",
    "                                                            'Microvoltages': 'Microvoltages',\n",
    "                                                            'LVH': 'LVH',\n",
    "                                                            'LAFB , LVH': 'LVH',\n",
    "                                                            'Microvoltages , BF': 'Microvoltages',\n",
    "                                                            'Microvoltages , RBBB': 'Microvoltages',\n",
    "                                                            'Microvoltages , LAFB': 'Microvoltages',\n",
    "                                                            'LVH , BF': 'LVH',\n",
    "                                                            'LVH , RBBB': 'LVH',\n",
    "                                                            'LVH , LBBB': 'LVH'\n",
    "                                                        }))\n",
    "    target_inclusion = ['Control','LVH','Microvoltages']\n",
    "    DATA = DATA.loc[DATA['Diagnosis'].isin(target_inclusion)]    \n",
    "\n",
    "    Reduction_map = {'Microvoltages': 'Abnormal', \n",
    "                     'LVH': 'Abnormal',\n",
    "                     'Control': 'Control'}\n",
    "else:\n",
    "    raise ValueError(f'Unknown target {TARGET}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df88667be8a69e6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:08.133688Z"
    }
   },
   "outputs": [],
   "source": [
    "if MULTI_CLASS==False:\n",
    "    DATA.loc[:, TargetCol] = DATA[TargetCol].map(Reduction_map)\n",
    "DATA = DATA.dropna(subset=[TargetCol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddc59f592300e9f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:08.334705Z"
    }
   },
   "outputs": [],
   "source": [
    "Infreq_cat_dict = {morphology_columns[k]:list(inf_cats) for k, inf_cats in enumerate(OneHot.infrequent_categories_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e647cddbf1921ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:08.521206Z"
    }
   },
   "outputs": [],
   "source": [
    "json.dump(Infreq_cat_dict, open(os.path.join(output_dir, 'infrequent_categories_map.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e34320264c5225",
   "metadata": {},
   "source": [
    "# Make tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "705c65e4096a8ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:08.738210Z"
    }
   },
   "outputs": [],
   "source": [
    "TreeKwargs = {\n",
    "    'criterion':'gini', \n",
    "    'splitter':'best', \n",
    "    'max_depth':5, \n",
    "    'min_samples_split':10, \n",
    "    'min_samples_leaf': 5, \n",
    "    'min_weight_fraction_leaf':0.05, \n",
    "    'max_features':None, \n",
    "    'random_state':7, \n",
    "    'max_leaf_nodes':50,\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "xgboost_kwargs = {\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': 6,\n",
    "    'max_leaves': 50,\n",
    "    'learning_rate': 2e-3,\n",
    "    'gamma': 0.4,\n",
    "    'subsample': 0.55,\n",
    "    'colsample_bytree':0.85,\n",
    "    'reg_alpha': 0.005,\n",
    "}\n",
    "# use OvR for LR\n",
    "logistic_kwargs = {\n",
    "    'penalty': 'elasticnet', \n",
    "    'solver': 'saga', \n",
    "    'dual': False, \n",
    "    'tol': 0.0001, \n",
    "    'C':1.0, \n",
    "    'fit_intercept': True, \n",
    "    'intercept_scaling':1, \n",
    "    'class_weight':None, \n",
    "    'random_state':7,     \n",
    "    'max_iter':5000, \n",
    "    'verbose': 0, \n",
    "    'warm_start': False, \n",
    "    'n_jobs':-1, \n",
    "    'l1_ratio':0.5\n",
    "}\n",
    "\n",
    "# If multiclass, use label_binarize and OvR for all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17a47f3113df9286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:08.923699Z"
    }
   },
   "outputs": [],
   "source": [
    "Splitter = RepeatedStratifiedKFold(n_splits=num_splits, \n",
    "                                   n_repeats=num_repeats, \n",
    "                                   random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a68d125a4d2c9de5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:09.110202Z"
    }
   },
   "outputs": [],
   "source": [
    "X = DATA.loc[:,[c for c in keep_columns if c!=TargetCol]]\n",
    "Y = DATA[TargetCol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e0fe0a87ee2e48a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:09.296704Z"
    }
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lbe = LabelEncoder()\n",
    "lb.fit(Y)\n",
    "lbe.fit(Y)\n",
    "TargetMap = {k:v for k,v in enumerate(lbe.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c944ee82a01ff1fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:09.483209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Abnormal', 'Control'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TargetMap.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f04bf0803ff7350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:09.668212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbe.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50139353f8bb4626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:09.869712Z"
    }
   },
   "outputs": [],
   "source": [
    "X.to_parquet(os.path.join(output_dir, f'data{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72e244d313d5c294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:11:11.035231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "for i, (train_index, test_index) in tqdm.tqdm(enumerate(Splitter.split(X, Y)),\n",
    "                                              total=num_splits * num_repeats):\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    y_train_encoded = lbe.transform(Y_train)\n",
    "    y_test_encoded = lbe.transform(Y_test)\n",
    "    \n",
    "    # We use standard scaling to effectively apply KNN imputation with distance weighting\n",
    "    Scaler = StandardScaler()\n",
    "    Scaler.fit(X_train)\n",
    "    X_train_scaled = pd.DataFrame(data=Scaler.transform(X_train), columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(Scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    Imputer = KNNImputer(add_indicator=MISSINGNESS_INDICATOR, n_neighbors=10, weights='distance')\n",
    "    Imputer.fit(X_train_scaled)\n",
    "    \n",
    "    X_train_imputed_scaled = Imputer.transform(X_train_scaled)\n",
    "    X_test_imputed_scaled = Imputer.transform(X_test_scaled)\n",
    "    \n",
    "    X_train_imputed = Scaler.inverse_transform(X_train_imputed_scaled)\n",
    "    X_test_imputed = Scaler.inverse_transform(X_test_imputed_scaled)\n",
    "    \n",
    "    X_train_imputed = pd.DataFrame(data=X_train_imputed,\n",
    "                                   columns=Imputer.get_feature_names_out())\n",
    "    \n",
    "    X_test_imputed = pd.DataFrame(data=X_test_imputed,\n",
    "                                   columns=Imputer.get_feature_names_out())\n",
    "    \n",
    "    clf = ctree.CustomDecisionTree(custom_rules=processed_rules,\n",
    "                             prune_threshold=None,\n",
    "                             Tree_kwargs=TreeKwargs,\n",
    "                             TargetMap = TargetMap, \n",
    "                             tot_max_depth=5)\n",
    "    if processed_rules is not None:\n",
    "        #print(\"Training custom tree...\")\n",
    "        clf.fit(X_train_imputed, y_train_encoded)\n",
    "        enriched_rules = clf.get_enriched_rules()\n",
    "        final_tree = clf.get_custom_rules_model()\n",
    "        \n",
    "        #############################\n",
    "        ## Writing out the tree #####\n",
    "        #############################\n",
    "        \n",
    "        if WRITE_OUT:\n",
    "            if processed_rules is not None:\n",
    "                json.dump(final_tree, \n",
    "                          open(os.path.join(output_dir, f\"tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.json\"), mode='w'))\n",
    "                ctree.update_html(tree=final_tree, \n",
    "                                  html_path=\"../src/treeTemplate.html\", \n",
    "                                  output_path=os.path.join(output_dir, f\"tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.html\"))\n",
    "                \n",
    "        cust_probas_train = clf.predict_proba(X_train_imputed)\n",
    "        cust_probas_test = clf.predict_proba(X_test_imputed)\n",
    "\n",
    "\n",
    "    clf_base = DecisionTreeClassifier(**TreeKwargs)\n",
    "    clf_xgb = XGBClassifier(**xgboost_kwargs)\n",
    "    clf_logistic = LogisticRegression(**logistic_kwargs)\n",
    "    \n",
    "    #print(\"Training classifiers...\")\n",
    "    #print(\"Training standard decision tree...\")\n",
    "    clf_base.fit(X_train_imputed, y_train_encoded)\n",
    "    #print(\"Training xgboost...\")\n",
    "    clf_xgb.fit(X_train_imputed, y_train_encoded)\n",
    "    #print(\"Training logistic regression...\")\n",
    "    clf_logistic.fit(X_train_imputed, y_train_encoded)\n",
    "    \n",
    "    Fold = i % num_splits\n",
    "    Repeat = i // num_splits\n",
    "    \n",
    "    #############################\n",
    "    ## Writing out the tree #####\n",
    "    #############################\n",
    "    \n",
    "    sklearn_tree = clf.load_from_sklearn_tree(clf_base, X_train_imputed, y_train_encoded)\n",
    "    final_tree_sklearn = sklearn_tree.get_custom_rules_model()\n",
    "    if WRITE_OUT:\n",
    "        json.dump(final_tree_sklearn, \n",
    "                  open(os.path.join(output_dir, f\"sklearn_tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.json\"), mode='w'))\n",
    "        ctree.update_html(tree=final_tree_sklearn, \n",
    "                         html_path=\"../src/treeTemplate.html\", \n",
    "                         output_path=os.path.join(output_dir, f\"sklearn_tree_Fold{Fold}_{Repeat}{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.html\"))\n",
    "        #############################\n",
    "    #############################\n",
    "    #############################    \n",
    "\n",
    "    \n",
    "    base_probas_train = clf_base.predict_proba(X_train_imputed)\n",
    "    base_probas_test = clf_base.predict_proba(X_test_imputed)\n",
    "    \n",
    "    xgb_probas_train = clf_xgb.predict_proba(X_train_imputed)\n",
    "    xgb_probas_test = clf_xgb.predict_proba(X_test_imputed)\n",
    "    \n",
    "    logistic_probas_train = clf_logistic.predict_proba(X_train_imputed)\n",
    "    logistic_probas_test = clf_logistic.predict_proba(X_test_imputed)\n",
    "    \n",
    "    result_df['indices'] = np.hstack([train_index, test_index])\n",
    "    result_df['Fold'] = Fold\n",
    "    result_df['Repeat'] = Repeat\n",
    "    result_df['Y_true'] = np.hstack([Y_train.values, Y_test.values])\n",
    "    result_df[[f'Y_pred_normalDT_{cname}' for cname in TargetMap.values()]] = np.vstack([base_probas_train, base_probas_test])\n",
    "    if processed_rules is not None:\n",
    "        result_df[[f'Y_pred_customDT_{cname}' for cname in TargetMap.values()]] = np.vstack([cust_probas_train, cust_probas_test])\n",
    "    result_df[[f'Y_pred_XGB_{cname}' for cname in TargetMap.values()]] = np.vstack([xgb_probas_train, xgb_probas_test])\n",
    "    result_df[[f'Y_pred_LR_{cname}' for cname in TargetMap.values()]] = np.vstack([logistic_probas_train, logistic_probas_test])    \n",
    "    result_df['Dataset'] = ['train' for _ in train_index]+['test' for _ in test_index]\n",
    "    \n",
    "    results_list.append(result_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bb23977f46fe04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:23:25.423823Z"
    }
   },
   "outputs": [],
   "source": [
    "Final_results = pd.concat(results_list, axis=0, ignore_index=True)\n",
    "Fina_results = Final_results.reset_index(drop=True)\n",
    "\n",
    "classes = set([c.split(\"_\")[-1] for c in Final_results.columns if 'pred' in c])\n",
    "\n",
    "for _class in classes:\n",
    "    Final_results[f'Y_true_{_class}'] = (Final_results['Y_true'] == _class).astype(int)\n",
    "\n",
    "Final_results.to_csv(\n",
    "    os.path.join(output_dir, f\"results{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.csv\"),\n",
    "    index=False, sep=\";\")\n",
    "\n",
    "Final_results.to_parquet(\n",
    "    os.path.join(output_dir, f\"results{MULTI_CLASS_STRING}{MISSINGNESS_INDICATOR_STRING}{MORPHO_MAP_STRING}.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4546e88efb16d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:45:27.368548700Z",
     "start_time": "2024-09-12T14:23:30.615895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Repeat</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_pred_normalDT_Abnormal</th>\n",
       "      <th>Y_pred_normalDT_Control</th>\n",
       "      <th>Y_pred_XGB_Abnormal</th>\n",
       "      <th>Y_pred_XGB_Control</th>\n",
       "      <th>Y_pred_LR_Abnormal</th>\n",
       "      <th>Y_pred_LR_Control</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Y_true_Abnormal</th>\n",
       "      <th>Y_true_Control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.142591</td>\n",
       "      <td>0.857409</td>\n",
       "      <td>0.314047</td>\n",
       "      <td>0.685953</td>\n",
       "      <td>0.195791</td>\n",
       "      <td>0.804209</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>0.680958</td>\n",
       "      <td>0.319042</td>\n",
       "      <td>0.484305</td>\n",
       "      <td>0.515695</td>\n",
       "      <td>0.815688</td>\n",
       "      <td>0.184312</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.142591</td>\n",
       "      <td>0.857409</td>\n",
       "      <td>0.324005</td>\n",
       "      <td>0.675995</td>\n",
       "      <td>0.150330</td>\n",
       "      <td>0.849670</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.366972</td>\n",
       "      <td>0.633028</td>\n",
       "      <td>0.355542</td>\n",
       "      <td>0.644458</td>\n",
       "      <td>0.502827</td>\n",
       "      <td>0.497173</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.142591</td>\n",
       "      <td>0.857409</td>\n",
       "      <td>0.313636</td>\n",
       "      <td>0.686364</td>\n",
       "      <td>0.192383</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>1051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.555356</td>\n",
       "      <td>0.444644</td>\n",
       "      <td>0.408325</td>\n",
       "      <td>0.591675</td>\n",
       "      <td>0.356346</td>\n",
       "      <td>0.643654</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>1053</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.275791</td>\n",
       "      <td>0.724209</td>\n",
       "      <td>0.388772</td>\n",
       "      <td>0.611228</td>\n",
       "      <td>0.244493</td>\n",
       "      <td>0.755507</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>1054</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>0.532585</td>\n",
       "      <td>0.467415</td>\n",
       "      <td>0.461405</td>\n",
       "      <td>0.538595</td>\n",
       "      <td>0.898628</td>\n",
       "      <td>0.101372</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>1055</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.532585</td>\n",
       "      <td>0.467415</td>\n",
       "      <td>0.408999</td>\n",
       "      <td>0.591001</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.371186</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>1056</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Control</td>\n",
       "      <td>0.532585</td>\n",
       "      <td>0.467415</td>\n",
       "      <td>0.328150</td>\n",
       "      <td>0.671850</td>\n",
       "      <td>0.191777</td>\n",
       "      <td>0.808223</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2124 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      indices  Fold  Repeat    Y_true  Y_pred_normalDT_Abnormal  \\\n",
       "0           0     0       0   Control                  0.142591   \n",
       "1           2     0       0  Abnormal                  0.680958   \n",
       "2           4     0       0   Control                  0.142591   \n",
       "3           5     0       0   Control                  0.366972   \n",
       "4           6     0       0   Control                  0.142591   \n",
       "...       ...   ...     ...       ...                       ...   \n",
       "2119     1051     1       0   Control                  0.555356   \n",
       "2120     1053     1       0   Control                  0.275791   \n",
       "2121     1054     1       0  Abnormal                  0.532585   \n",
       "2122     1055     1       0   Control                  0.532585   \n",
       "2123     1056     1       0   Control                  0.532585   \n",
       "\n",
       "      Y_pred_normalDT_Control  Y_pred_XGB_Abnormal  Y_pred_XGB_Control  \\\n",
       "0                    0.857409             0.314047            0.685953   \n",
       "1                    0.319042             0.484305            0.515695   \n",
       "2                    0.857409             0.324005            0.675995   \n",
       "3                    0.633028             0.355542            0.644458   \n",
       "4                    0.857409             0.313636            0.686364   \n",
       "...                       ...                  ...                 ...   \n",
       "2119                 0.444644             0.408325            0.591675   \n",
       "2120                 0.724209             0.388772            0.611228   \n",
       "2121                 0.467415             0.461405            0.538595   \n",
       "2122                 0.467415             0.408999            0.591001   \n",
       "2123                 0.467415             0.328150            0.671850   \n",
       "\n",
       "      Y_pred_LR_Abnormal  Y_pred_LR_Control Dataset  Y_true_Abnormal  \\\n",
       "0               0.195791           0.804209   train                0   \n",
       "1               0.815688           0.184312   train                1   \n",
       "2               0.150330           0.849670   train                0   \n",
       "3               0.502827           0.497173   train                0   \n",
       "4               0.192383           0.807617   train                0   \n",
       "...                  ...                ...     ...              ...   \n",
       "2119            0.356346           0.643654    test                0   \n",
       "2120            0.244493           0.755507    test                0   \n",
       "2121            0.898628           0.101372    test                1   \n",
       "2122            0.628814           0.371186    test                0   \n",
       "2123            0.191777           0.808223    test                0   \n",
       "\n",
       "      Y_true_Control  \n",
       "0                  1  \n",
       "1                  0  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "...              ...  \n",
       "2119               1  \n",
       "2120               1  \n",
       "2121               0  \n",
       "2122               1  \n",
       "2123               1  \n",
       "\n",
       "[2124 rows x 13 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2d6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argus-pipeline-4qerB8WK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
